<p align="center"> 
  <img src="reports/figs/logo_azul.png" alt="CEFET-MG" width="100px" height="100px">
</p>

<h1 align="center"> Trabalho PrÃ¡tico de InteligÃªncia Artificial (2025/2) </h1>

Este repositÃ³rio contÃ©m as implementaÃ§Ãµes dos trabalhos prÃ¡ticos da disciplina de IA (CEFET-MG). O projeto estÃ¡ dividido em partes independentes, abordando Ã¡rvores de decisÃ£o manuais e algoritmos de aprendizado de mÃ¡quina supervisionado.

## ğŸš€ Como Reproduzir

### PrÃ©-requisitos
* Python 3.10 ou superior
* Gerenciador de pacotes `pip`
* `make` para automaÃ§Ã£o (nativo no Linux/WSL)

### InstalaÃ§Ã£o RÃ¡pida (via Makefile)
No terminal (Linux/WSL), execute:

```bash
make setup
```

-----

## ğŸŒ³ Parte 1: Ãrvore de DecisÃ£o Manual

ImplementaÃ§Ã£o de uma Ã¡rvore de decisÃ£o "hard-coded" (sem bibliotecas de ML) com tema livre.

  * **Tema:** Carreira em ProgramaÃ§Ã£o.
  * **Objetivo:** 10 perguntas binÃ¡rias que sugerem uma linguagem de programaÃ§Ã£o ou stack baseada nas preferÃªncias do usuÃ¡rio.

### ExecuÃ§Ã£o

Para rodar a Ã¡rvore interativa:

```bash
make part1
```

### DocumentaÃ§Ã£o

  * [Diagrama da Ãrvore (Mermaid)](src/part1_tree_manual/tree_diagram.md)

  ![imagem](reports/figs/tree-1.svg)

-----

## ğŸ¤– Parte 2: Aprendizado Supervisionado (Olist)

AplicaÃ§Ã£o e comparaÃ§Ã£o de algoritmos de classificaÃ§Ã£o (KNN, SVM, Ãrvore de DecisÃ£o) no dataset pÃºblico de E-Commerce brasileiro (Olist).

  * **Problema:** ClassificaÃ§Ã£o BinÃ¡ria.
  * **Target:** Prever se um pedido serÃ¡ **entregue com atraso** (`is_late = 1`).
  * **Dataset:** [Olist Brazilian E-Commerce](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce).

### EstratÃ©gia de Dados

O dataset original apresenta um desbalanceamento severo (\~92% dos pedidos sÃ£o entregues no prazo). Para permitir que os modelos aprendessem a identificar atrasos, foi utilizada a tÃ©cnica de **Undersampling (Balanceamento)**:

  * Foram selecionados **todos** os casos de atraso disponÃ­veis.
  * Foi selecionada uma amostra aleatÃ³ria de casos "no prazo" de mesmo tamanho.
  * **Resultado:** Treinamento realizado com uma proporÃ§Ã£o de 50/50, maximizando o *Recall* da classe de atrasos.

### ExecuÃ§Ã£o

Siga a ordem abaixo para reproduzir os resultados:

```bash
make part2
# Executa preprocessamento + treinamento
```

### Resultados Obtidos

Os modelos apresentaram uma **AcurÃ¡cia mÃ©dia de \~60%** apÃ³s o balanceamento. Embora a acurÃ¡cia global tenha diminuÃ­do em comparaÃ§Ã£o ao modelo desbalanceado (que apenas "chutava" a classe majoritÃ¡ria), o **Recall (RevocaÃ§Ã£o) para atrasos subiu significativamente**, tornando os modelos funcionalmente Ãºteis para detectar problemas logÃ­sticos.

-----
## ğŸ§¬ Parte 3: Algoritmo GenÃ©tico (OtimizaÃ§Ã£o)

ImplementaÃ§Ã£o de um Algoritmo GenÃ©tico (AG) **do zero** (sem bibliotecas de GA) para otimizar os hiperparÃ¢metros do SVM da Parte 2.

### DefiniÃ§Ã£o do Problema

O objetivo Ã© encontrar a melhor combinaÃ§Ã£o de `C` e `Gamma` para maximizar a acurÃ¡cia do SVM.

  * **Gene 1 (C):** Penalidade de erro (Busca no intervalo `[0.1, 100]`).
  * **Gene 2 (Gamma):** Coeficiente do Kernel RBF (Busca no intervalo `[0.0001, 1.0]`).

### Detalhes da ImplementaÃ§Ã£o

  * **CodificaÃ§Ã£o:** Real-valued (Vetor de float).
  * **Fitness:** AcurÃ¡cia do SVM treinado em uma amostra balanceada de 2.000 instÃ¢ncias (para eficiÃªncia).
  * **Operadores GenÃ©ticos:**
      * **SeleÃ§Ã£o:** Torneio.
      * **Crossover:** AritmÃ©tico (MÃ©dia ponderada).
      * **MutaÃ§Ã£o:** Gaussiana (AdiÃ§Ã£o de ruÃ­do controlado).
      * **Elitismo:** PreservaÃ§Ã£o dos 2 melhores indivÃ­duos (Top-2).

### ExecuÃ§Ã£o

```bash
make part3
# ou: python src/part3_ga/run_tuning.py
```

### AnÃ¡lise dos Resultados

O algoritmo demonstrou convergÃªncia rÃ¡pida (geralmente na 3Âª geraÃ§Ã£o) para:

  * **Gamma â‰ˆ 0.0001** (Limite inferior).
  * **AcurÃ¡cia â‰ˆ 59-60%**.

**ConclusÃ£o:** O AG "descobriu" que, devido ao ruÃ­do nos dados do Olist, a melhor estratÃ©gia Ã© simplificar a fronteira de decisÃ£o (Gamma baixo -\> modelo quase linear), evitando *overfitting*. A estagnaÃ§Ã£o em 60% confirma que este Ã© o limite preditivo intrÃ­nseco das features disponÃ­veis.

-----

## ğŸ› ï¸ Estrutura do Projeto

```
ia-trabalho-2025-2/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # Arquivos CSV originais (Olist)
â”‚   â””â”€â”€ processed/      # Dados processados (.npy) para treino
â”œâ”€â”€ reports/
|   â”œâ”€â”€ figs # figuras geradas pro relatÃ³rio/readme 
|   â””â”€â”€ part2_ml
â”‚     â”œâ”€â”€ metrics.csv     # Tabela comparativa de resultados parte 2
â”‚     â””â”€â”€ metrics_details.txt # RelatÃ³rios detalhados (Matriz de ConfusÃ£o) da parte 2
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/         # UtilitÃ¡rios de sistema e reprodutibilidade (Seeds)
â”‚   â”œâ”€â”€ part1_tree_manual/
â”‚   â”‚   â”œâ”€â”€ tree_manual.py
â”‚   â”‚   â””â”€â”€ tree_diagram.md
â”‚   â”œâ”€â”€ part2_ml/
â”‚   â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”‚   â”œâ”€â”€ train_knn.py
â”‚   â”‚   â”œâ”€â”€ train_svm.py
â”‚   â”‚   â”œâ”€â”€ train_tree.py
â”‚   â”‚   â””â”€â”€ utils_metrics.py
â”‚   â””â”€â”€ part3_ga/
â”‚       â”œâ”€â”€ ga.py
â”‚       â””â”€â”€ run_tuning.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ‘¥ Autores

  * **Rafael (Radsfer)** - Engenharia de ComputaÃ§Ã£o (CEFET-MG)
